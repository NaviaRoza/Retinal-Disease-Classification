{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2736749,"sourceType":"datasetVersion","datasetId":1629109}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-23T16:50:24.929665Z","iopub.execute_input":"2023-12-23T16:50:24.930075Z","iopub.status.idle":"2023-12-23T16:50:25.350841Z","shell.execute_reply.started":"2023-12-23T16:50:24.930037Z","shell.execute_reply":"2023-12-23T16:50:25.349458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nimport glob\nimport itertools\nimport os\nimport random\nimport shutil\n\nimport cv2\nimport h5py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom IPython.display import display\nfrom PIL import Image as im\nfrom sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n                             confusion_matrix, precision_score, recall_score)\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import (BatchNormalization, Concatenate, Conv2D,\n                                     Conv2DTranspose, Dropout, Input,\n                                     MaxPool2D)\nfrom tensorflow.keras.losses import (CategoricalCrossentropy,\n                                     SparseCategoricalCrossentropy)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:51:21.020590Z","iopub.execute_input":"2023-12-23T16:51:21.021288Z","iopub.status.idle":"2023-12-23T16:51:21.031232Z","shell.execute_reply.started":"2023-12-23T16:51:21.021249Z","shell.execute_reply":"2023-12-23T16:51:21.029983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam, SGD\nfrom sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, matthews_corrcoef\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:51:48.174888Z","iopub.execute_input":"2023-12-23T16:51:48.175313Z","iopub.status.idle":"2023-12-23T16:51:48.181725Z","shell.execute_reply.started":"2023-12-23T16:51:48.175275Z","shell.execute_reply":"2023-12-23T16:51:48.180298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = '/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/train'\ntest = '/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/test'\nval = '/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/val'\n#rescale parameter equal to 1/255 to normalize these values\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define data augmentation parameters\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=10,       # Randomly rotate images by up to 20 degrees\n    width_shift_range=0.2,   # Randomly shift images horizontally by up to 20% of the width\n    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the height\n    shear_range=0.2,         # Apply random shear transformations\n    zoom_range=0.2,          # Randomly zoom into images\n    fill_mode='nearest'      # Fill in newly created pixels during augmentation\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create data generators with augmentation\ntraining_set = train_datagen.flow_from_directory(\n    directory=train,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='sparse',\n    shuffle= True,\n)\n\ntest_set = test_datagen.flow_from_directory(\n    directory=test,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='sparse',\n    shuffle= False,\n)\n\nval_set = val_datagen.flow_from_directory(\n    directory=val,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='sparse',\n    shuffle= False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:52:11.157192Z","iopub.execute_input":"2023-12-23T16:52:11.157596Z","iopub.status.idle":"2023-12-23T16:52:33.217677Z","shell.execute_reply.started":"2023-12-23T16:52:11.157560Z","shell.execute_reply":"2023-12-23T16:52:33.216527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get a batch of images from the training data generator\nbatch_images, batch_labels = training_set.next()\n\n# Plot four samples\nplt.figure(figsize=(10, 10))\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(batch_images[i])\n    plt.title(f\"Class: {batch_labels[i]}\")\n    plt.axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:53:12.464129Z","iopub.execute_input":"2023-12-23T16:53:12.464537Z","iopub.status.idle":"2023-12-23T16:53:13.965874Z","shell.execute_reply.started":"2023-12-23T16:53:12.464504Z","shell.execute_reply":"2023-12-23T16:53:13.964685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get a batch of images from the training data generator\nbatch_images, batch_labels = training_set.next()\n\n# Plot four samples\nplt.figure(figsize=(10, 10))\nfor i in range(4):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(batch_images[i])\n    plt.title(f\"Class: {batch_labels[i]}\")\n    plt.axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:53:33.955087Z","iopub.execute_input":"2023-12-23T16:53:33.955486Z","iopub.status.idle":"2023-12-23T16:53:35.333979Z","shell.execute_reply.started":"2023-12-23T16:53:33.955455Z","shell.execute_reply":"2023-12-23T16:53:35.332934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:53:54.654801Z","iopub.execute_input":"2023-12-23T16:53:54.655786Z","iopub.status.idle":"2023-12-23T16:53:54.662297Z","shell.execute_reply.started":"2023-12-23T16:53:54.655737Z","shell.execute_reply":"2023-12-23T16:53:54.661345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes=8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Flatten, ReLU,Dropout\nfrom tensorflow.keras.applications import VGG19,DenseNet121\ndef cbam_block(x, channels, reduction_ratio=16):\n    # Spatial attention\n    spatial_avg = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n    spatial_max = tf.reduce_max(x, axis=[1, 2], keepdims=True)\n    spatial_attention = tf.keras.layers.Conv2D(channels // reduction_ratio, kernel_size=1, strides=1, padding='same')(spatial_avg)\n    spatial_attention = tf.keras.layers.ReLU()(spatial_attention)\n    spatial_attention = tf.keras.layers.Conv2D(channels, kernel_size=1, strides=1, padding='same')(spatial_attention)\n    spatial_attention = tf.keras.layers.Activation('sigmoid')(spatial_attention)\n\n    # Channel attention\n    channel_avg = tf.reduce_mean(x, axis=[3], keepdims=True)\n    channel_max = tf.reduce_max(x, axis=[3], keepdims=True)\n    channel_attention = tf.keras.layers.Dense(channels // reduction_ratio)(channel_avg)\n    channel_attention = tf.keras.layers.ReLU()(channel_attention)\n    channel_attention = tf.keras.layers.Dense(channels)(channel_attention)\n    channel_attention = tf.keras.layers.Activation('sigmoid')(channel_attention)\n\n    # Apply attention\n    x = x * spatial_attention + x * channel_attention\n\n    return x\ninput_shape = (224, 224, 3)\ndef build_res_cbam(input_shape, num_classes):\n    input_tensor = tf.keras.layers.Input(shape=input_shape)\n    base_model = DenseNet121(weights='imagenet', include_top=False, input_tensor=input_tensor)\n    \n    x = cbam_block(base_model.output, channels=1024) # Note: DenseNet121 has 1024 output channels\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(256, activation='relu')(x)\n    # Add dropout layers\n    x = tf.keras.layers.Dropout(0.5)(x) # You can adjust the dropout rate as needed\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    #x = tf.keras.layers.Flatten()\n    x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=input_tensor, outputs=x)\n    \n    return model\n\n# Specify input size and number of classes\n\n# Build the DenseNet121CBAM model\n#input_shape = (image_size,image_size, 3)\nmodel = build_res_cbam(input_shape, num_classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.optimizers import Adam, SGD","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.layers import Input, Dense, Flatten, ReLU,Dropout\n# from tensorflow.keras.applications import VGG19,ResNet50\n# def cbam_block(x, channels, reduction_ratio=16):\n#     # Spatial attention\n#     spatial_avg = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n#     spatial_max = tf.reduce_max(x, axis=[1, 2], keepdims=True)\n#     spatial_attention = tf.keras.layers.Conv2D(channels // reduction_ratio, kernel_size=1, strides=1, padding='same')(spatial_avg)\n#     spatial_attention = tf.keras.layers.ReLU()(spatial_attention)\n#     spatial_attention = tf.keras.layers.Conv2D(channels, kernel_size=1, strides=1, padding='same')(spatial_attention)\n#     spatial_attention = tf.keras.layers.Activation('sigmoid')(spatial_attention)\n\n#     # Channel attention\n#     channel_avg = tf.reduce_mean(x, axis=[1, 2], keepdims=True)  # Change from [3] to [1, 2]\n#     channel_max = tf.reduce_max(x, axis=[1, 2], keepdims=True)  # Change from [3] to [1, 2]\n#     channel_attention = tf.keras.layers.Conv2D(channels // reduction_ratio, kernel_size=1, strides=1, padding='same')(channel_avg)\n#     channel_attention = tf.keras.layers.ReLU()(channel_attention)\n#     channel_attention = tf.keras.layers.Conv2D(channels, kernel_size=1, strides=1, padding='same')(channel_attention)\n#     channel_attention = tf.keras.layers.Activation('sigmoid')(channel_attention)\n\n#     # Apply attention\n#     x = x * spatial_attention + x * channel_attention\n\n#     return x\n\n# input_shape = (224, 224, 3)\n\n# Vgg = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# # CBAM VGG19\n# ratio = 16\n\n# input_layer = Input(shape=input_shape)\n# out = Vgg.layers[1](input_layer)  # Block one of VGG19\n# out = Vgg.layers[2](out)\n# out = Vgg.layers[3](out)\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[4](out)  # Block two of VGG19\n# out = Vgg.layers[5](out)\n# out = Vgg.layers[6](out)\n\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[7](out)  # Block three of VGG19\n# out = Vgg.layers[8](out)\n# out = Vgg.layers[9](out)\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[10](out)\n# out = Vgg.layers[11](out)\n# out = Vgg.layers[12](out)  # Block four of VGG19\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[13](out)\n# #out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[14](out)\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[15](out)\n# #out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[16](out)\n\n# #out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[17](out)  # Block five of VGG19\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[18](out)\n# #out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[19](out)\n# out = Vgg.layers[20](out)\n# out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# #out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# out = Vgg.layers[21](out)\n\n# #out = cbam_block(out, channels=out.shape[-1], reduction_ratio=ratio)\n# flatten = Flatten()(out)\n# out = Dense(100)(flatten)\n# out = Dropout(0.5)(out) \n# out = ReLU()(out)\n\n# out = Dense(100)(out)\n# out = Dropout(0.5)(out) \n# out = ReLU()(out)\n# out = Dense(8)(out)\n# out = tf.keras.layers.Softmax()(out)\n\n# model = Model(input_layer, out)\n# # Set initial learning rate for Adam\n# initial_lr_adam = 0.0001\n# adam_optimizer1 = Adam(learning_rate=initial_lr_adam)\n# opt = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.001)\n# # Set learning rate for SGD\nlr_sgd = 0.0001\nsgd_optimizer = SGD(learning_rate=lr_sgd)\n\n# #------\n# initial_learning_rate = 0.01\n# first_decay_steps = 1000\n# lr_decayed_fn = (\n#   tf.keras.optimizers.schedules.CosineDecayRestarts(\n#       initial_learning_rate,\n#       first_decay_steps))\n# sgd_optimizer2 = SGD(learning_rate=lr_decayed_fn)\n# # Compile the model with the Adam optimizer\nmodel.compile(optimizer=sgd_optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T19:43:08.659860Z","iopub.execute_input":"2023-12-23T19:43:08.660162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\nhistory = model.fit(\n    x=training_set,  \n    validation_data= val_set, \n    epochs=100\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:56:11.301331Z","iopub.execute_input":"2023-12-23T16:56:11.301743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[6,4])\nplt.plot(history.history['accuracy'], 'black', linewidth=2.0)\nplt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Accuracy', fontsize=10)\nplt.title('Accuracy Curves', fontsize=12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[6,4])\nplt.plot(history.history['loss'], 'black', linewidth=2.0)\nplt.plot(history.history['val_loss'], 'blue', linewidth=2.0)\nplt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Loss', fontsize=10)\nplt.title('Loss Curves', fontsize=12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\n# Get predictions\nvalidation_steps_per_epoch = np.math.ceil(val_set.samples / val_set.batch_size)\npredictions = model.predict_generator(val_set, steps=validation_steps_per_epoch)\n\n# Get the most likely class\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Get the true classes\ntrue_classes = val_set.classes\n\n# Get the class labels\nclass_labels = list(val_set.class_indices.keys())\n\n# Print the classification report\nreport = classification_report(true_classes, predicted_classes, target_names=class_labels)\nprint(report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Calculate the confusion matrix\nconf_mat = confusion_matrix(true_classes, predicted_classes)\n\n# Plot the confusion matrix\nplt.figure(figsize=(len(class_labels), len(class_labels)))\nsns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n#     plt.figure(figsize= (10, 10))\n#     plt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n#     plt.title(title)\n#     plt.colorbar()\n#     tick_marks = np.arange(len(classes))\n#     plt.xticks(tick_marks, classes, rotation= 45)\n#     plt.yticks(tick_marks, classes)\n#     if normalize:\n#         cm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n#         print('Normalized Confusion Matrix')\n#     else:\n#         print('Confusion Matrix, Without Normalization')\n#     print(cm)\n#     thresh = cm.max() / 2.\n#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n#         plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n#     plt.tight_layout()\n#     plt.ylabel('True Label')\n#     plt.xlabel('Predicted Label')\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = model.predict_generator(test_set)\n# y_pred = np.argmax(preds, axis=1)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target_names = ['AMD', 'CNV', 'CSR', 'DME', 'DR', 'DRUSEN', 'MH', 'Normal']\n# # Confusion matrix\n# cm = confusion_matrix(test_set.classes, y_pred)\n# plot_confusion_matrix(cm= cm, classes= target_names, title = 'Confusion Matrix')\n# # Classification report\n# print(classification_report(test_set.classes, y_pred, target_names= target_names))","metadata":{},"execution_count":null,"outputs":[]}]}